## 원핫인코딩

+ 특성에 들어있는 고유한 값마다 새로운 dummy 특성을 만드는 방법이다.
+ 원핫인코딩을 통해 범주를 벡터화하여 해당 튜플에서 해당하는 범주만 1, 나머지 범주는 0을 갖도록 할 수 있다.
  + 범주형 변수의 범주가 4개라면 총 4개의 열이 생성된다.
  + ex) 빨, 주, 노, 초이며 현재 튜플이 빨강이라면 빨강만 1, 나머지는 0의 값을 갖는다.
+ 하지만, 이렇게 되면 빨, 주, 노, 초의 열들의 값이 항상 다 더하면 1이 되는 관계를 갖는다.
  + 이러한 선형 관계 때문에 생기는 문제가 **다중공선성**이다.
+ 이를 해결하기 위해서는 위와 같은 `빨 + 주 + 노 + 초 = 1`이라는 관계를 없애줘야 한다.
  + 이때 사용하는 방법이 범주를 벡터화한 열 중 하나를 없애는 것이다. (주로 첫 번째 것을 없앤다. drop_first=True)
  + 즉, 주, 노, 초로만 나눠 놓고 주, 노, 초가 전부 0인 경우 이 튜플이 빨강임을 알 수 있기 때문에 데이터의 문제는 없으면서 선형 관계가 생기지 않는다.
  + https://towardsdatascience.com/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a
  + https://dnai-deny.tistory.com/12
+ 따라서, 결과적으로 범주의 개수가 N개인 범주형 변수를 적절하게 원핫이코딩하면 총 N-1개의 열이 생성된다.

### 어려움

+ 원핫인코딩 시에는 다중공선성을 신경써야 한다는 것
+ 원핫인코딩으로 인해 범주가 너무 많이 나뉘면 칼럼이 많이 생기기 때문에 메모리 문제, 모델이 제대로 해당 범주형 변수를 반영하지 못한다는 문제(구글링 결과 범주가 너무 많으면 모델에서 학습에 별로 영향을 안미치는 변수로 취급한다고 함)
+ 원핫인코딩 자체가 다중선형회귀에 적합하지 않을 수도 있다는 것
  + 따라서, 실제 데이터로 train / test 셋을 나눠서 평가를 해봐야 한다.

<br/>

## 예시 코딩

### 로지스틱 회귀

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
flight = ['대한', '아시아나', '아시아나', '대한', '대한', '진에어', '아시아나', '진에어', '진에어', '진에어', '대한', '아시아나', '아시아나', '대한', '아시아나', '아시아나', '대한', '대한', '대한', '진에어', '진에어', '진에어', '진에어', '진에어', '진에어', '대한', '아시아나', '대한', '아시아나'] * 100000
weather = ['맑음', '비', '눈', '낙뢰', '맑음', '맑음', '맑음', '비', '눈', '눈', '비', '비', '낙뢰', '낙뢰', '눈', '맑음', '맑음', '눈', '비', '맑음', '눈', '비', '낙뢰', '맑음', '비', '비', '눈', '맑음', '낙뢰'] * 100000
late = [0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1] * 100000
cols = ['flight', 'weather', 'late']
df = pd.DataFrame(list(map(list, zip(flight, weather, late))), columns=cols)
# 다중공선성 제거를 위해 pd.get_dummies에서 첫 범주를 삭제
train = pd.get_dummies(df, drop_first=True)
# 0번째 열이 '지연여부'로 들어감
X = train[train.columns[1:]].to_numpy()
Y = train[train.columns[0]].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(X, Y, random_state=42)
lr = LogisticRegression()
lr.fit(train_input, train_target)
np.set_printoptions(precision=6, suppress=True)
print(train_input)
print(lr.predict(train_input[:]))
# [0 0 1 ... 1 0 0]
print(lr.predict_proba(train_input[:]))
# [[0.999865 0.000135]
#  [0.999884 0.000116]
#  [0.000009 0.999991]
#  ...
#  [0.000133 0.999867]
#  [0.999865 0.000135]
#  [0.999856 0.000144]]
print(lr.score(train_input, train_target))
# 0.9654850574712643
print(lr.score(test_input, test_target))
# 0.9656137931034483
```

<br/>

### 다중선형회귀

```python
from sklearn.linear_model import LinearRegression
flight = ['대한', '아시아나', '아시아나', '대한', '대한', '진에어', '아시아나', '진에어', '진에어', '진에어', '대한', '아시아나', '아시아나', '대한', '아시아나', '아시아나', '대한', '대한', '대한', '진에어', '진에어', '진에어', '진에어', '진에어', '진에어', '대한', '아시아나', '대한', '아시아나'] * 1000000
weather = ['맑음', '비', '눈', '낙뢰', '맑음', '맑음', '맑음', '비', '눈', '눈', '비', '비', '낙뢰', '낙뢰', '눈', '맑음', '맑음', '눈', '비', '맑음', '눈', '비', '낙뢰', '맑음', '비', '비', '눈', '맑음', '낙뢰'] * 1000000
late = [5, 15, 30, 60, 3, 4, 10, 20, 25, 35, 17, 10, 50, 100, 50, 10, 2, 25, 13, 0, 20, 12, 45, 6, 17, 17, 28, 3, 30] * 1000000
cols = ['flight', 'weather', 'late']
df = pd.DataFrame(list(map(list, zip(flight, weather, late))), columns=cols)
print(df[:29])
#    flight weather  late
# 0      대한      맑음     5
# 1    아시아나       비    15
# 2    아시아나       눈    30
# 3      대한      낙뢰    60
# ...
train = pd.get_dummies(df, drop_first=True)
X = train[train.columns[1:]].to_numpy()
Y = train[train.columns[0]].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(X, Y, random_state=42)
lr = LinearRegression()
lr.fit(train_input, train_target)
print(lr.predict(train_input[:]))
# [18.83109933 60.39705499  2.7542878  ... 18.83109933  8.07223988 12.49248053]
print(lr.score(train_input, train_target))
# 0.7488027412083975
print(lr.score(test_input, test_target))
# 0.7488202375624603
```

